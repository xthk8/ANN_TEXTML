{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Amazon Product Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맨 처음 - 데이터 읽어오기\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def read_reviews_from_folders(base_path):\n",
    "    if not os.path.exists(base_path):\n",
    "        raise ValueError(f\"The specified base path does not exist: {base_path}\")\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    for score in range(1, 6):\n",
    "        folder_path = os.path.join(base_path, str(score))\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Warning: Folder does not exist: {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        for file_path in glob.glob(os.path.join(folder_path, '*.txt')):\n",
    "            with open(file_path, 'r', encoding='utf8') as file:\n",
    "                review_text = file.read().strip()\n",
    "                if review_text:\n",
    "                    data.append(review_text)\n",
    "                    labels.append(score)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "review_base_path = \"C:/Users/USER/Desktop/학부연구/밑바닥부터 시작하는 딥러닝/reviews3\"\n",
    "\n",
    "reviews, scores = read_reviews_from_folders(review_base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 활성화 함수 정의\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a - np.max(a, axis=1, keepdims=True))\n",
    "    sum_exp_a = np.sum(exp_a, axis=1, keepdims=True)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "# 신경망 초기화 함수\n",
    "def initialize_network(input_size, hidden_size1, hidden_size2, output_size):\n",
    "    np.random.seed(42)\n",
    "    return {\n",
    "        'W1': np.random.rand(input_size, hidden_size1),\n",
    "        'b1': np.random.rand(hidden_size1),\n",
    "        'W2': np.random.rand(hidden_size1, hidden_size2),\n",
    "        'b2': np.random.rand(hidden_size2),\n",
    "        'W3': np.random.rand(hidden_size2, output_size),\n",
    "        'b3': np.random.rand(output_size)\n",
    "    }\n",
    "\n",
    "# 순전파 함수\n",
    "def forward(network, x):\n",
    "    W1, b1 = network['W1'], network['b1']\n",
    "    W2, b2 = network['W2'], network['b2']\n",
    "    W3, b3 = network['W3'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "    return y\n",
    "\n",
    "def loss(x, t):\n",
    "    y = forward(x)\n",
    "\n",
    "def numerical_gradient(x, t):\n",
    "    loss_W = lambda W: loss(x, t)\n",
    "\n",
    "    grads = {}\n",
    "    grads['W1'] = numerical_gradient(loss_W, network['W1'])\n",
    "    grads['b1'] = numerical_gradient(loss_W, network['b1'])\n",
    "    grads['W2'] = numerical_gradient(loss_W, network['W2'])\n",
    "    grads['b2'] = numerical_gradient(loss_W, network['b2'])\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.1926666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 데이터셋 분할\n",
    "train_reviews, test_reviews, train_scores, test_scores = train_test_split(\n",
    "    reviews, scores, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 텍스트 토큰화 및 패딩\n",
    "tokenizer = Tokenizer(num_words=2000)  # 상위 2000개 단어만 사용\n",
    "tokenizer.fit_on_texts(train_reviews)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_reviews)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_reviews)\n",
    "\n",
    "# 모든 시퀀스를 같은 길이로 맞춤 (예: 100 단어)\n",
    "maxlen = 25\n",
    "train_data = pad_sequences(train_sequences, maxlen=maxlen)\n",
    "test_data = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "\n",
    "# 원-핫 인코딩 적용\n",
    "encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "encoder.fit(train_data)\n",
    "train_vectors = encoder.transform(train_data).toarray()\n",
    "test_vectors = encoder.transform(test_data).toarray()\n",
    "\n",
    "# 신경망 구조 설정 및 초기화\n",
    "input_size = 25\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 5\n",
    "output_size = 5\n",
    "\n",
    "network = initialize_network(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# 정확도 평가 함수\n",
    "def predict_scores(network, x):\n",
    "    y = forward(network, x)\n",
    "    predicted_scores = np.argmax(y, axis=1) + 1\n",
    "    return predicted_scores\n",
    "\n",
    "def calculate_accuracy(network, x, actual_scores):\n",
    "    predicted_scores = predict_scores(network, x)\n",
    "    return np.mean(predicted_scores == actual_scores)\n",
    "\n",
    "# 랜덤 샘플링 및 평가\n",
    "def random_sample_and_evaluate(network, train_data, train_scores, test_data, test_scores, num_samples=30, sample_size=100):\n",
    "    accuracies = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # 학습 데이터셋에서 랜덤하게 샘플링\n",
    "        train_indices = np.random.choice(len(train_data), sample_size, replace=False)\n",
    "        sampled_train_data = train_data[train_indices]\n",
    "        sampled_train_scores = np.array(train_scores)[train_indices]\n",
    "\n",
    "        # 테스트 데이터셋에서 랜덤하게 샘플링\n",
    "        test_indices = np.random.choice(len(test_data), sample_size, replace=False)\n",
    "        sampled_test_data = test_data[test_indices]\n",
    "        sampled_test_scores = np.array(test_scores)[test_indices]\n",
    "\n",
    "        # 이 부분에서 모델을 학습시킬 수 있습니다.\n",
    "        # 예: train_model(network, sampled_train_data, sampled_train_scores)\n",
    "\n",
    "        # 테스트 데이터셋으로 정확도 계산\n",
    "        accuracy = calculate_accuracy(network, sampled_test_data, sampled_test_scores)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # 평균 정확도 계산\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    return mean_accuracy\n",
    "\n",
    "# 랜덤 샘플링 및 평가\n",
    "mean_accuracy = random_sample_and_evaluate(network, train_data, train_scores, test_data, test_scores)\n",
    "print(\"Mean Accuracy:\", mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "정확도 분석\n",
    "\n",
    "- 데이터의 불균형 존재 가능\n",
    "- 데이터 전처리 과정에서의 부적절함\n",
    "- 파라미터 설정의 무작위성 (적절한 학습이 아님)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
