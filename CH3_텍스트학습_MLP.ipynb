{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 활성화함수가 미분함수(비선형함수)여야 하는 이유는?\n",
    "\n",
    "- 선형함수는 '은닉층이 없는 네트워크'와 같음\n",
    "- 선형함수는 출력이 입력의 상수배만큼 변함 (정형적, 연속성이 적음)\n",
    "- 비선형함수는 비정형적, 연속적\n",
    "- 은닉층(여러 개의 층의 이점)을 이용하기 위해서는 비선형함수를 이용해야 함\n",
    "\n",
    "2. 지도학습의 분류와 회귀가 무엇인가?\n",
    "    1) 분류와 회귀의 데이터(자료) 구조와 유형은 각각 어떻게 되는가?\n",
    "\n",
    "- 분류\n",
    "    - 분류는 데이터가 어떤 클래스(분야, 종류 등)에 속하는지를 결정하는 문제\n",
    "    - 분류의 데이터 : 하나 이상의 특징을 가지며, 각각 명확한 범주(Class)에 속하는 데이터\n",
    "    - 분류의 출력 : 이산적인(명확한) 클래스 레이블 또는 클래스에 속할 확률\n",
    "    - 분류의 유형 : 이진 분류(Binary Classification), 다중 분류(Multi-Class Classification)\n",
    "\n",
    "- 회귀\n",
    "    - 회귀는 입력 데이터에서 (연속적인) 수치를 예측하는 문제\n",
    "    - 회귀의 데이터 : 하나 이상의 특징을 가지며 연속적인 값을 가지는 데이터\n",
    "    - 회귀의 출력 : 연속적인 수치 값\n",
    "    - 회귀의 유형 : 단일 회귀(Simple Regression), 다중 회귀(Multiple Regression)\n",
    "\n",
    "3. 분류 3층 신경망을 파이썬으로 구현하고 해당 구조를 만족하는 Input data를 찾아서(최소 10,000개 이상이며, 독립변수와 종속변수를 모두 가지고 있는 데이터를 찾을 것) 신경망에 입력한 결과값을 도출하시오 (층별 가중치와 Bias는 0~1사이 숫자를 랜덤 생성할 것)\n",
    "    1) 신경망 구조\n",
    "        - 입력층의 뉴런 25개, 첫번째 은닉층의 뉴런 50개, 두번째 은닉층의 뉴런 5개, 출력층의 뉴런 5개 (아래 구조 참조)\n",
    "        - 출력층 뉴런의 활성화 함수는 softmax, 은닉층 뉴런의 활성화 함수는 sigmoid\n",
    "    ![image.png](attachment:image.png)\n",
    "    - 아래 코드 구현\n",
    "\n",
    "4. 찾은 Input data를 100개씩 랜덤 샘플링을 30회 시행하여 위 신경망에 대한 평균 Accuracy를 구하고 왜 그러한 값이 도출되었는지 설명하시오.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Amazon Product Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맨 처음 - 데이터 읽어오기\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def read_reviews_from_folders(base_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for score in range(1, 6):  \n",
    "        folder_path = os.path.join(base_path, str(score))\n",
    "        # 각각의 점수에 해당하는 텍스트 파일 읽기\n",
    "        for file_path in glob.glob(os.path.join(folder_path, '*.txt')):\n",
    "            with open(file_path, 'r', encoding='utf8') as file:\n",
    "                review_text = file.read().strip()\n",
    "                data.append(review_text)\n",
    "                labels.append(score)\n",
    "    return data, labels\n",
    "\n",
    "review_base_path = 'C:/Users/USER/Desktop/학부연구/밑바닥부터 시작하는 딥러닝/Review/reviews' \n",
    "\n",
    "reviews, scores = read_reviews_from_folders(review_base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 활성화 함수 정의\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a - np.max(a, axis=1, keepdims=True))\n",
    "    sum_exp_a = np.sum(exp_a, axis=1, keepdims=True)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "# 신경망 초기화 함수\n",
    "def initialize_network(input_size, hidden_size1, hidden_size2, output_size):\n",
    "    np.random.seed(42)\n",
    "    return {\n",
    "        'W1': np.random.rand(input_size, hidden_size1),\n",
    "        'b1': np.random.rand(hidden_size1),\n",
    "        'W2': np.random.rand(hidden_size1, hidden_size2),\n",
    "        'b2': np.random.rand(hidden_size2),\n",
    "        'W3': np.random.rand(hidden_size2, output_size),\n",
    "        'b3': np.random.rand(output_size)\n",
    "    }\n",
    "\n",
    "# 순전파 함수\n",
    "def forward(network, x):\n",
    "    W1, b1 = network['W1'], network['b1']\n",
    "    W2, b2 = network['W2'], network['b2']\n",
    "    W3, b3 = network['W3'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "    return y\n",
    "\n",
    "def loss(x, t):\n",
    "    y = forward(x)\n",
    "\n",
    "def numerical_gradient(x, t):\n",
    "    loss_W = lambda W: loss(x, t)\n",
    "\n",
    "    grads = {}\n",
    "    grads['W1'] = numerical_gradient(loss_W, network['W1'])\n",
    "    grads['b1'] = numerical_gradient(loss_W, network['b1'])\n",
    "    grads['W2'] = numerical_gradient(loss_W, network['W2'])\n",
    "    grads['b2'] = numerical_gradient(loss_W, network['b2'])\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy over 30 samples: 0.19433333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 데이터셋 분할\n",
    "train_reviews, test_reviews, train_scores, test_scores = train_test_split(\n",
    "    reviews, scores, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 텍스트 토큰화 및 패딩\n",
    "tokenizer = Tokenizer(num_words=2000)  # 상위 2000개 단어만 사용\n",
    "tokenizer.fit_on_texts(train_reviews)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_reviews)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_reviews)\n",
    "\n",
    "# 모든 시퀀스를 같은 길이로 맞춤 (예: 100 단어)\n",
    "maxlen = 25\n",
    "train_data = pad_sequences(train_sequences, maxlen=maxlen)\n",
    "test_data = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "\n",
    "# 원-핫 인코딩 적용\n",
    "encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "encoder.fit(train_data)\n",
    "train_vectors = encoder.transform(train_data).toarray()\n",
    "test_vectors = encoder.transform(test_data).toarray()\n",
    "\n",
    "# 신경망 구조 설정 및 초기화\n",
    "input_size = 25\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 5\n",
    "output_size = 5\n",
    "\n",
    "network = initialize_network(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# 정확도 평가 함수\n",
    "def predict_scores(network, x):\n",
    "    y = forward(network, x)\n",
    "    predicted_scores = np.argmax(y, axis=1) + 1\n",
    "    return predicted_scores\n",
    "\n",
    "def calculate_accuracy(network, x, actual_scores):\n",
    "    predicted_scores = predict_scores(network, x)\n",
    "    return np.mean(predicted_scores == actual_scores)\n",
    "\n",
    "# 랜덤 샘플링 및 평가\n",
    "def random_sample_and_evaluate(network, train_data, train_scores, test_data, test_scores, num_samples=30, sample_size=100):\n",
    "    accuracies = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # 학습 데이터셋에서 랜덤하게 샘플링\n",
    "        train_indices = np.random.choice(len(train_data), sample_size, replace=False)\n",
    "        sampled_train_data = train_data[train_indices]\n",
    "        sampled_train_scores = np.array(train_scores)[train_indices]\n",
    "\n",
    "        # 테스트 데이터셋에서 랜덤하게 샘플링\n",
    "        test_indices = np.random.choice(len(test_data), sample_size, replace=False)\n",
    "        sampled_test_data = test_data[test_indices]\n",
    "        sampled_test_scores = np.array(test_scores)[test_indices]\n",
    "\n",
    "        # 이 부분에서 모델을 학습시킬 수 있습니다.\n",
    "        # 예: train_model(network, sampled_train_data, sampled_train_scores)\n",
    "\n",
    "        # 테스트 데이터셋으로 정확도 계산\n",
    "        accuracy = calculate_accuracy(network, sampled_test_data, sampled_test_scores)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # 평균 정확도 계산\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    return mean_accuracy\n",
    "\n",
    "# 랜덤 샘플링 및 평가\n",
    "mean_accuracy = random_sample_and_evaluate(network, train_data, train_scores, test_data, test_scores)\n",
    "print(\"Mean Accuracy over 30 samples:\", mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "정확도 분석\n",
    "\n",
    "- 데이터의 불균형 존재 가능\n",
    "- 데이터 전처리 과정에서의 부적절함\n",
    "- 파라미터 설정의 무작위성 (적절한 학습이 아님)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
